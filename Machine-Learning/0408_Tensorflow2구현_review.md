# KNN êµ¬í˜„í•˜ê¸° ğŸ˜ƒ
- BMI ì˜ˆì œ(multinomial classification)
- Logistic Regressionì˜ ê²°ê³¼ì™€ ë¹„êµ


```python
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Raw Data Loading
df = pd.read_csv('./data/bmi.csv', skiprows=3)

# Data Split
train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(df[['height', 'weight']],
                 df['label'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['label'])

# ê²°ì¸¡ì¹˜ X, ì´ìƒì¹˜ X
# ì •ê·œí™” ì§„í–‰
scaler = MinMaxScaler()
scaler.fit(train_x_data)
norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)

# LogisticRegression êµ¬í˜„
model = LogisticRegression()
model.fit(norm_train_x_data, train_t_data)
acc = model.score(norm_test_x_data, test_t_data)  # scoring ê¸°ë³¸ê°’ì´ accuracy
print(f'LogisticRegressionì˜ Accuracy : {acc}')

# KNNìœ¼ë¡œ êµ¬í˜„
knn_classifier = KNeighborsClassifier(n_neighbors=3)
knn_classifier.fit(norm_train_x_data, train_t_data)
acc = knn_classifier.score(norm_test_x_data, test_t_data)
print(f'KNNì˜ Accuracy : {acc}')
```

    LogisticRegressionì˜ Accuracy : 0.9851666666666666
    KNNì˜ Accuracy : 0.9985
    

# OzoneëŸ‰ ì˜ˆì¸¡ Linear Regression êµ¬í˜„í•˜ê¸° ğŸ˜†
- Tensorflow 2.x
- ë°ì´í„° ì „ì²˜ë¦¬


```python
%reset

import numpy as np
import pandas as pd
from scipy import stats
from sklearn.neighbors import KNeighborsRegressor  # ì—°ì†ì ì¸ ìˆ«ìê°’ ì˜ˆì¸¡
from sklearn.preprocessing import MinMaxScaler
import warnings

warnings.filterwarnings(action='ignore')
```

    Once deleted, variables cannot be recovered. Proceed (y/[n])? y
    


```python
# Raw Data Loading
df = pd.read_csv('./data/ozone.csv')
display(df.head())

x_data = df[['Solar.R', 'Wind', 'Temp']]  # DataFrame(2ì°¨ì›)
t_data = df['Ozone']                      # Series(1ì°¨ì›)
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Ozone</th>
      <th>Solar.R</th>
      <th>Wind</th>
      <th>Temp</th>
      <th>Month</th>
      <th>Day</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>41.0</td>
      <td>190.0</td>
      <td>7.4</td>
      <td>67</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36.0</td>
      <td>118.0</td>
      <td>8.0</td>
      <td>72</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12.0</td>
      <td>149.0</td>
      <td>12.6</td>
      <td>74</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18.0</td>
      <td>313.0</td>
      <td>11.5</td>
      <td>62</td>
      <td>5</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>14.3</td>
      <td>56</td>
      <td>5</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>


## 1. ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ Missing Valueë¥¼ ì°¾ì•„ medianìœ¼ë¡œ imputation
- np.nanmedian : nanì„ ì œì™¸í•˜ê³  median ê°’ êµ¬í•¨


```python
x_data.isnull().sum()
```




    Solar.R    7
    Wind       0
    Temp       0
    dtype: int64




```python
for col in x_data.columns:
    col_median = np.nanmedian(x_data[col])
    x_data[col].loc[x_data[col].isnull()] = col_median
    
x_data.isnull().sum()
```




    Solar.R    0
    Wind       0
    Temp       0
    dtype: int64



## 2. ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ì´ìƒì¹˜ë¥¼ ê²€ì¶œí•œ í›„ ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ê°’ë“¤ì˜ meanìœ¼ë¡œ ì´ìƒì¹˜ ëŒ€ì²´


```python
zscore_threshold = 2.0

for col in x_data.columns:
    outlier = x_data[col][np.abs(stats.zscore(x_data[col])) > zscore_threshold]
    col_mean = np.mean(x_data.loc[~x_data[col].isin(outlier),col])
    x_data.loc[x_data[col].isin(outlier), col] = col_mean
```

## 3. ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ì´ìƒì¹˜ë¥¼ ê²€ì¶œí•œ í›„ ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ê°’ë“¤ì˜ meanìœ¼ë¡œ ì´ìƒì¹˜ ëŒ€ì²´


```python
outlier = t_data[np.abs(stats.zscore(t_data)) > zscore_threshold]
col_mean = np.mean(~t_data.isin(outlier))
t_data[t_data.isin(outlier)] = col_mean
```

## 4. ì •ê·œí™” ì§„í–‰


```python
scaler_x = MinMaxScaler()
scaler_t = MinMaxScaler()

scaler_x.fit(x_data.values)  
scaler_t.fit(t_data.values.reshape(-1,1))  # scalerëŠ” 2ì°¨ì› ndarrayë¡œ ì‚¬ìš©í•´ì•¼ í•¨

norm_x_data = scaler_x.transform(x_data.values)
norm_t_data = scaler_t.transform(t_data.values.reshape(-1,1)).ravel()
```

## 5. ì¢…ì†ë³€ìˆ˜ì˜ Missing ValueëŠ” KNNì„ ì´ìš©í•´ì„œ ì˜ˆì¸¡ê°’ ì‚¬ìš©


```python
# ì¢…ì†ë³€ìˆ˜ê°€ Missing Valueê°€ ì•„ë‹Œ ë…ë¦½ë³€ìˆ˜ë“¤ê³¼ ì¢…ì†ë³€ìˆ˜ë“¤ì„ ì¶”ì¶œ(KNNì„ í•™ìŠµí•˜ê¸° ìœ„í•´)
norm_train_x_data = norm_x_data[~np.isnan(norm_t_data)]
norm_train_t_data = norm_t_data[~np.isnan(norm_t_data)]

# ëª¨ë¸ ìƒì„±
knn_regressor = KNeighborsRegressor(n_neighbors=2)
knn_regressor.fit(norm_train_x_data, norm_train_t_data)

# ì¢…ì†ë³€ìˆ˜ê°€ Missing Valueì¸ ë…ë¦½ë³€ìˆ˜ë“¤ì„ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ê°’ ì˜ˆì¸¡
knn_predict = knn_regressor.predict(norm_x_data[np.isnan(norm_t_data)])
norm_t_data[np.isnan(norm_t_data)] = knn_predict
```

- ìµœì¢…ì ì¸ ë°ì´í„° norm_x_data, norm_t_data

### sklearn êµ¬í˜„ & tensorflow 2.x êµ¬í˜„


```python
from sklearn.linear_model import LinearRegression
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import SGD

test_data = np.array([[330, 15, 80]])  # íƒœì–‘ê´‘ 330, ë°”ëŒ 15, ì˜¨ë„ 80

# sklearn êµ¬í˜„
model = LinearRegression()
model.fit(norm_x_data, norm_t_data)
result = model.predict(scaler_x.transform(test_data))

print('sklearn ì˜ˆì¸¡ê°’ : {}'.format(scaler_t.inverse_transform(result.reshape(-1,1))))

# Tensorflow 2.x êµ¬í˜„(Linear Regression)
keras_model = Sequential()

keras_model.add(Flatten(input_shape=(3,)))   # Input Layer
keras_model.add(Dense(units=1,
                      activation='linear'))  # output Layer

keras_model.compile(optimizer=SGD(learning_rate=1e-2),
                    loss='mse')

keras_model.fit(norm_x_data,
                norm_t_data,
                epochs=5000,
                verbose=0)

result = keras_model.predict(scaler_x.transform(test_data))
print('tensorflow ì˜ˆì¸¡ê°’ : {}'.format(scaler_t.inverse_transform(result.reshape(-1,1))))
```

    sklearn ì˜ˆì¸¡ê°’ : [[36.93077619]]
    tensorflow ì˜ˆì¸¡ê°’ : [[36.97865]]
    

# Logistic Regression ğŸ˜
- binary classificationì„ skearnê³¼ tensorflow 2.xë¡œ êµ¬í˜„
- titanic ë°ì´í„°(Kaggleì—ì„œ ë‹¤ìš´ë¡œë“œ)


```python
%reset

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import SGD
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# ì‹¤ì œ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì— ì´ìƒì¹˜ê°€ ê²€ì¶œë˜ë”ë¼ë„ í•´ë‹¹ ê°’ ì‚¬ìš©
```

    Once deleted, variables cannot be recovered. Proceed (y/[n])? y
    


```python
# Raw Data Loading
df = pd.read_csv('./data/kaggle/titanic/train.csv')

# í•™ìŠµí•˜ê¸° ì¢‹ì€ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬

# í•„ìš”ì—†ëŠ” column(feature) ì‚­ì œ (ìƒê´€ê´€ê³„ ë¶„ì„)
df = df.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], 
             axis=1,
             inplace=False)

# í•˜ë‚˜ë¡œ í•©ì¹  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ì€ í•©ì¹¨
df['Family'] = df['SibSp'] + df['Parch']
df = df.drop(['SibSp', 'Parch'], axis=1, inplace=False)
display(df.head())
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>Embarked</th>
      <th>Family</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>C</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>S</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



```python
df.isnull().sum()
```




    Survived      0
    Pclass        0
    Sex           0
    Age         177
    Embarked      2
    Family        0
    dtype: int64




```python
# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# `Embarked` columnì€ ê²°ì¸¡ì¹˜ê°€ 2ê°œ
# => ìµœë¹ˆê°’ì„ ì´ìš©í•´ì„œ missing value ì±„ì›€ - ì—¬ê¸°ì„œëŠ” Q
df['Embarked'] = df['Embarked'].fillna('Q')

# `Age` columnì˜ ê²°ì¸¡ì¹˜ëŠ” í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
df['Age'] = df['Age'].fillna(df['Age'].mean())
```


```python
# ë¬¸ìë¡œ ë˜ì–´ ìˆëŠ” ê°’ì€ ìˆ«ìë¡œ ë³€ê²½
gender_string = { 'male': 0, 'female': 1 }
df['Sex'] = df['Sex'].map(gender_string)

embarked_string = { 'S': 0, 'C': 1, 'Q': 2 }
df['Embarked'] = df['Embarked'].map(embarked_string)

def age_category(age):
    if((age >= 0) & (age < 25)):
        return 0
    elif ((age >= 25) & (age < 50)):
        return 1
    else:
        return 2
    
df['Age'] = df['Age'].map(age_category)

df.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>Embarked</th>
      <th>Family</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Data Split
train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(df.drop('Survived', axis=1, inplace=False),
                 df['Survived'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['Survived'])

# Normalization
scaler = MinMaxScaler()
scaler.fit(train_x_data)

norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)
```


```python
# sklearn êµ¬í˜„
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(norm_train_x_data, train_t_data)
sklearn_result = model.score(norm_test_x_data, test_t_data)
print(f'sklearn ì •í™•ë„ : {sklearn_result}')
```

    sklearn ì •í™•ë„ : 0.7873134328358209
    


```python
# tensorflow êµ¬í˜„
keras_model = Sequential()
keras_model.add(Flatten(input_shape=(5,)))
keras_model.add(Dense(units=1,
                      activation='sigmoid'))
keras_model.compile(optimizer=SGD(learning_rate=1e-2),
                    loss='binary_crossentropy',
                    metrics=['accuracy'])
keras_model.fit(norm_train_x_data,
                train_t_data,
                epochs=1000,
                verbose=0)

keras_result = keras_model.evaluate(norm_test_x_data, test_t_data)
print(f'TF2.x ì •í™•ë„ : {keras_result}')  
#                    loss                accuracy
```

    9/9 [==============================] - 0s 445us/step - loss: 0.4660 - accuracy: 0.7948
    TF2.x ì •í™•ë„ : [0.46601223945617676, 0.7947761416435242]
    
