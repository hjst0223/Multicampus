# Logistic Regressionì„ Graphicí•˜ê²Œ ì•Œì•„ë³´ê¸° ğŸ§


```python
import numpy as np
from sklearn import linear_model
import mglearn   # utility module
import matplotlib.pyplot as plt
import warnings  # warning ì¶œë ¥ë˜ì§€ ì•Šê²Œ

warnings.filterwarnings(action='ignore')  # ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ

# Training Data Set - xëŠ” ë…ë¦½ë³€ìˆ˜, yëŠ” ì¢…ì†ë³€ìˆ˜ 
x, y = mglearn.datasets.make_forge()  # make_forge(): íŠ¹ì • datasetì´ ì£¼ì–´ì§
```


```python
print(x)  # ë…ë¦½ë³€ìˆ˜(2ì°¨ì› ndarray)
```

    [[ 9.96346605  4.59676542]
     [11.0329545  -0.16816717]
     [11.54155807  5.21116083]
     [ 8.69289001  1.54322016]
     [ 8.1062269   4.28695977]
     [ 8.30988863  4.80623966]
     [11.93027136  4.64866327]
     [ 9.67284681 -0.20283165]
     [ 8.34810316  5.13415623]
     [ 8.67494727  4.47573059]
     [ 9.17748385  5.09283177]
     [10.24028948  2.45544401]
     [ 8.68937095  1.48709629]
     [ 8.92229526 -0.63993225]
     [ 9.49123469  4.33224792]
     [ 9.25694192  5.13284858]
     [ 7.99815287  4.8525051 ]
     [ 8.18378052  1.29564214]
     [ 8.7337095   2.49162431]
     [ 9.32298256  5.09840649]
     [10.06393839  0.99078055]
     [ 9.50048972 -0.26430318]
     [ 8.34468785  1.63824349]
     [ 9.50169345  1.93824624]
     [ 9.15072323  5.49832246]
     [11.563957    1.3389402 ]]
    


```python
print(y)  # ì¢…ì†ë³€ìˆ˜(1ì°¨ì› ndarray) 
```

    [1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0]
    


```python
mglearn.discrete_scatter(x[:,0], x[:,1], y)
plt.legend(['class 0', 'class 1'], loc='best')  # ë²”ë¡€
plt.show()
```


    
![png](/Machine-Learning/images/0401/output_4_0.png)
    


## ê²½ê³„ì„ ì„ êµ¬í•˜ê¸° ìœ„í•´ linear regressionì„ êµ¬í•œë‹¤.
## => ì´ ê²½ê³„ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ ì–´ëŠ ìª½ì— ì†í•´ìˆëŠ”ì§€ ì°¾ëŠ” ê²ƒì´ logistic regression


```python
mglearn.discrete_scatter(x[:,0], x[:,1], y)
plt.legend(['class 0', 'class 1'], loc='best')  # ë²”ë¡€

model = linear_model.LinearRegression()

# ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì„ ê°ê° 2ì°¨ì›ìœ¼ë¡œ reshape
model.fit(x[:,0].reshape(-1,1), x[:,1].reshape(-1,1)) 

# coef_ëŠ” 2ì°¨ì› í˜•íƒœì´ë¯€ë¡œ ravel()ë¡œ 1ì°¨ì›ìœ¼ë¡œ ë°”ê¿”ì¤Œ
plt.plot(x[:,0], x[:,0]*model.coef_.ravel() + model.intercept_, color='r')  # (x, y(weight&bias))
plt.show()
```


    
![png](/Machine-Learning/images/0401/output_6_0.png)
    


### ì¢…ì†ë³€ìˆ˜ì— ë”°ë¼
- ì—°ì†ì ì¸ ê°’ => Linear Regression
- binary classification => Logistic Regression

# Q. ë¶„ë¥˜ ë¬¸ì œë¥¼ Linear Regressionìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ?


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

# Training Data Set
# ê³µë¶€ì‹œê°„ì— ë”°ë¥¸ í•©ê²© ì—¬ë¶€
x_data = np.array([1, 2, 5, 8, 10])  # ê³µë¶€ì‹œê°„ (ë…ë¦½ë³€ìˆ˜)
t_data = np.array([0, 0, 0, 1, 1])   # í•©ê²©ì—¬ë¶€ (ì¢…ì†ë³€ìˆ˜, 0: ë¶ˆí•©ê²©, 1: í•©ê²©)

# model ìƒì„±
model = linear_model.LinearRegression()
# model í•™ìŠµ
model.fit(x_data.reshape(-1,1), t_data.reshape(-1,1))  # Multiple Linear Regressionì˜ ê²½ìš°ë¥¼ ê³ ë ¤í•˜ì—¬ 2ì°¨ì›ìœ¼ë¡œ
# prediction
print(model.predict(np.array([[7]])))  # ê³µë¶€ì‹œê°„ì´ 7ì¼ ë•Œ (2ì°¨ì›)

plt.scatter(x_data, t_data)
plt.plot(x_data, x_data*model.coef_.ravel() + model.intercept_)
plt.show()
```

    [[0.63265306]]
    


    
![png](/Machine-Learning/images/0401/output_9_1.png)
    


## [[0.63265306]] -> 0.5ë³´ë‹¤ í¬ë¯€ë¡œ í•©ê²©ì¼ê¹Œ?

### ë°ì´í„° ë°”ê¿”ì„œ í•´ë³´ê¸° 


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

# Training Data Set
# ê³µë¶€ì‹œê°„ì— ë”°ë¥¸ í•©ê²© ì—¬ë¶€
x_data = np.array([1, 2, 5, 8, 10, 30])  # ê³µë¶€ì‹œê°„ (ë…ë¦½ë³€ìˆ˜)
t_data = np.array([0, 0, 0, 1, 1, 1])    # í•©ê²©ì—¬ë¶€ (ì¢…ì†ë³€ìˆ˜, 0: ë¶ˆí•©ê²©, 1: í•©ê²©)

# model ìƒì„±
model = linear_model.LinearRegression()
# model í•™ìŠµ
model.fit(x_data.reshape(-1,1), t_data.reshape(-1,1))  # Multiple Linear Regressionì˜ ê²½ìš°ë¥¼ ê³ ë ¤í•˜ì—¬ 2ì°¨ì›ìœ¼ë¡œ
# prediction
print(model.predict(np.array([[7]])))  # ê³µë¶€ì‹œê°„ì´ 7ì¼ ë•Œ (2ì°¨ì›)

plt.scatter(x_data, t_data)
plt.plot(x_data, x_data*model.coef_.ravel() + model.intercept_)
plt.show()
```

    [[0.41831972]]
    


    
![png](/Machine-Learning/images/0401/output_12_1.png)
    


## [[0.63265306]] => [[0.41831972]] ìœ„ë³´ë‹¤ ì‘ì•„ì§
## => Linear Regressionìœ¼ë¡œ íŒë‹¨/ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ì—ëŠ” ë¬´ë¦¬ê°€ ìˆë‹¤.

# Logistic Regressionì„ 3ê°€ì§€ í˜•íƒœë¡œ êµ¬í˜„í•˜ê¸° ğŸ¤”
- ê°„ë‹¨í•œ Training Data Set ì´ìš©

## 1. python êµ¬í˜„


```python
import numpy as np

# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def numerical_derivative(f,x):   
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)  
    
    # iteratorë¥¼ ì´ìš©í•´ì„œ ì…ë ¥ë³€ìˆ˜ xì— ëŒ€í•œ í¸ë¯¸ë¶„ ìˆ˜í–‰
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index 
        tmp = x[idx]            
        x[idx] = tmp + delta_x   
        fx_plus_delta = f(x)
        
        x[idx] = tmp - delta_x    
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
        
        x[idx] = tmp             
        it.iternext()
        
    return derivative_x

# Training Data Set
x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(-1,1)  # Linear Regressionì„ ìœ„í•´ 2ì°¨ì›ìœ¼ë¡œ
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)

# Weight, bias ì •ì˜
W = np.random.rand(1,1)
b = np.random.rand(1)

# logistic regression model, predict model, hypothesis
def predict(x):
    
    z = np.dot(x, W) + b          # linear regression model
    y = 1 / (1 + np.exp(-1 * z))  # logistic regression model
    
    result = 0
    
    # ê³„ì‚°ë˜ëŠ” yê°’ì€ 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ ê°’
    if y >= 0.5:
        result = 1
    else:
        result = 0
        
    return y, result

# Cross Entropy(log loss)
def loss_func(input_data):  # [W1, W2, W3, ... ,b]
    
    input_W = input_data[:-1].reshape(-1,1)
    input_b = input_data[-1]
    
    z = np.dot(x_data, input_W) + input_b
    y = 1 / (1 + np.exp(-1 * z))
    
    # yê°€ 0ì´ ë˜ë©´ ì•ˆë˜ë¯€ë¡œ ì½”ë“œì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ” ì•„ì£¼ ì‘ì€ ê°’ì„ ë”í•´ì¤€ë‹¤.
    delta = 1e-7
    
    # cross entroy
    return -1 * np.sum(t_data * np.log(y + delta) + (1 - t_data) * np.log(1 - y + delta))
    
# learning rate ì„¤ì •
learning_rate = 1e-4

# ë°˜ë³µí•™ìŠµ ì§„í–‰
for step in range(300000):
    
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)  # [W b]
    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)

    W = W - derivative_result[:-1].reshape(1,1)
    b = b - derivative_result[-1]
    
    if step % 30000 == 0:
        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
        print('W : {}, b:{}, loss:{}'.format(W, b, loss_func(input_param)))
```

    W : [[0.42800666]], b:[0.39021404], loss:20.7741281875545
    W : [[0.26384694]], b:[-3.10594358], loss:2.9262521867333553
    W : [[0.37742307]], b:[-4.62696479], loss:2.1337004280208545
    W : [[0.45386132]], b:[-5.64769496], loss:1.7817370701211435
    W : [[0.51312912]], b:[-6.43715213], loss:1.5720154025549722
    W : [[0.56232598]], b:[-7.091062], loss:1.4283522945679454
    W : [[0.60482481]], b:[-7.65492192], loss:1.3216139245243288
    W : [[0.64250818]], b:[-8.1541223], loss:1.23798743038805
    W : [[0.67654109]], b:[-8.60436806], loss:1.1699759996678094
    W : [[0.70769829]], b:[-9.01609655], loss:1.113112620097403
    


```python
# prediction
study_hour = np.array([[13]])  # 12ì‹œê°„ - ë¶ˆí•©ê²©, 14ì‹œê°„ - í•©ê²©
y_prob, result = predict(study_hour)
print('í•©ê²© í™•ë¥  : {}, í•©ê²©ì—¬ë¶€ : {}'.format(y_prob, result))
```

    í•©ê²© í™•ë¥  : [[0.5444273]], í•©ê²©ì—¬ë¶€ : 1
    

## 2. sklearn êµ¬í˜„


```python
from sklearn import linear_model

# Training Data Set
x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)

# model ìƒì„±
model = linear_model.LogisticRegression()

# model í•™ìŠµ
model.fit(x_data, t_data)

# prediction
study_hour = np.array([[13]])  # 12ì‹œê°„ - ë¶ˆí•©ê²©, 14ì‹œê°„ - í•©ê²©
result = model.predict(study_hour)  # ìµœì¢…ê²°ê³¼ë§Œ ì•Œë ¤ì¤Œ
result_prob = model.predict_proba(study_hour)  # ë¶ˆí•©ê²©, í•©ê²© í™•ë¥ 
print('í•©ê²© í™•ë¥  : {}, í•©ê²©ì—¬ë¶€ : {}'.format(result_prob, result))

# 0.49990609 => 0.5ì— ëª» ë¯¸ì¹¨
```

    í•©ê²© í™•ë¥  : [[0.50009391 0.49990609]], í•©ê²©ì—¬ë¶€ : [0]
    

## 3. Tensorflow êµ¬í˜„


```python
import tensorflow as tf

# Training Data Set
x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)

# placeholder
X = tf.placeholder(shape=[None,1], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([1,1]))
b = tf.Variable(tf.random.normal([1]))

# Model(Hypothesis)
logit = tf.matmul(X, W) + b  # linear regression model
H = tf.sigmoid(logit)        # logistic regression model

# loss function
# sigmoid_cross_entropy_with_logits():
# linear regression modelì„ ì´ìš©í•´ì„œ cross entropy ê°’ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜
# ë‘ ê°œì˜ ì¸ì í•„ìš” (logits, labels)
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels=T))
# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)

# Session & ì´ˆê¸°í™”
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# ë°˜ë³µí•™ìŠµ ì§„í–‰
for step in range(30000):
    
    _, W_val, b_val, loss_val = sess.run([train, W, b, loss],
                                         feed_dict={X: x_data,
                                                    T: t_data})
    if step % 3000 == 0:
        print('W : {}, b:{}, loss:{}'.format(W_val, b_val, loss_val))
```


    W : [[0.12902921]], b:[-0.9290906], loss:0.5256268382072449
    W : [[0.1322414]], b:[-1.3360596], loss:0.45846638083457947
    W : [[0.15852948]], b:[-1.6907678], loss:0.4162381589412689
    W : [[0.18189465]], b:[-2.005362], loss:0.38302838802337646
    W : [[0.20288943]], b:[-2.287653], loss:0.35629352927207947
    W : [[0.2219421]], b:[-2.5435991], loss:0.33431944251060486
    W : [[0.23938502]], b:[-2.7777612], loss:0.31592875719070435
    W : [[0.255477]], b:[-2.9936805], loss:0.30029383301734924
    W : [[0.27042216]], b:[-3.194123], loss:0.28682073950767517
    W : [[0.28438252]], b:[-3.3812919], loss:0.27507340908050537
    


```python
# prediction
study_hour = np.array([[13]])  # 12ì‹œê°„ - ë¶ˆí•©ê²©, 14ì‹œê°„ - í•©ê²©
result = sess.run(H, feed_dict={X: study_hour})
print('í•©ê²© í™•ë¥  : {}'.format(result))
```

    í•©ê²© í™•ë¥  : [[0.57698435]]
    
