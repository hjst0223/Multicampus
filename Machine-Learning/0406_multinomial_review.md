# Logistic Regression êµ¬í˜„í•˜ê¸° ğŸ˜¯
- ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” data set


```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data    # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target  # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Model ìƒì„±
model = linear_model.LogisticRegression()

# Model í•™ìŠµ
model.fit(train_x_data, train_t_data)

# accuracyë¡œ model í‰ê°€
test_score = model.score(test_x_data, test_t_data)

print('Logistic Regression Modelì˜ ì •í™•ë„ : {}'.format(test_score))
```

    Logistic Regression Modelì˜ ì •í™•ë„ : 0.9473684210526315
    

## SGD Classifier


```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data    # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target  # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Model ìƒì„±
sgd = linear_model.SGDClassifier(loss='log',  # logistic regression ì´ìš©
                                 tol=1e-5,    # ì–¼ë§ˆë‚˜ ë°˜ë³µí• ê±´ì§€ë¥¼ lossê°’ìœ¼ë¡œ ì„¤ì • 
                                 random_state=2)
# Model í•™ìŠµ
sgd.fit(train_x_data, train_t_data)

# Accuracy ì¸¡ì •
test_score = sgd.score(test_x_data, test_t_data)

print('SGDClassifierì˜ ì •í™•ë„ : {}'.format(test_score))
# 0.8947368421052632 ì •ê·œí™”í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ê° featureë§ˆë‹¤ scaleì´ ì œê°ê°ì„
```

    SGDClassifierì˜ ì •í™•ë„ : 0.8947368421052632
    

## ì •ê·œí™”ë¥¼ ì´ìš©í•œ SGD Classifier


```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data    # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target  # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Data ì •ê·œí™”
scaler = StandardScaler()
scaler.fit(train_x_data)

# Model ìƒì„±
sgd = linear_model.SGDClassifier(loss='log',  # logistic regression ì´ìš©
                                 tol=1e-5,    # ì–¼ë§ˆë‚˜ ë°˜ë³µí• ê±´ì§€ë¥¼ lossê°’ìœ¼ë¡œ ì„¤ì • 
                                 random_state=2)
# Model í•™ìŠµ
sgd.fit(scaler.transform(train_x_data), train_t_data)

# Accuracy ì¸¡ì •
test_score = sgd.score(scaler.transform(test_x_data), test_t_data)

print('ì •ê·œí™”ë¥¼ ì´ìš©í•œ SGDClassifierì˜ ì •í™•ë„ : {}'.format(test_score))
```

    ì •ê·œí™”ë¥¼ ì´ìš©í•œ SGDClassifierì˜ ì •í™•ë„ : 0.9649122807017544
    

## ì •ê·œí™”ì™€ L2 Regularizationì„ ì´ìš©í•œ SGD Classifier


```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data    # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target  # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Data ì •ê·œí™”
scaler = StandardScaler()
scaler.fit(train_x_data)

# Model ìƒì„±
sgd = linear_model.SGDClassifier(loss='log',    # logistic regression ì´ìš©
                                 tol=1e-5,      # ì–¼ë§ˆë‚˜ ë°˜ë³µí• ê±´ì§€ë¥¼ lossê°’ìœ¼ë¡œ ì„¤ì • 
                                 penalty='l2',  # L2 ê·œì œ ì´ìš© 
                                 alpha=0.001,   # ê·œì œ ê°•ë„     
                                 random_state=2)
# Model í•™ìŠµ
sgd.fit(scaler.transform(train_x_data), train_t_data)

# Accuracy ì¸¡ì •
test_score = sgd.score(scaler.transform(test_x_data), test_t_data)

print('ì •ê·œí™”ì™€ ê·œì œë¥¼ ì´ìš©í•œ SGDClassifierì˜ ì •í™•ë„ : {}'.format(test_score))
# 0.9649122807017544 => 0.9707602339181286 (ê·œì œë¥¼ ì´ìš©í•˜ë©´ ì¡°ê¸ˆ ë” ë‚˜ì€ ëª¨ë¸ ë§Œë“¤ ìˆ˜ ìˆìŒ)
```

    ì •ê·œí™”ì™€ ê·œì œë¥¼ ì´ìš©í•œ SGDClassifierì˜ ì •í™•ë„ : 0.9707602339181286
    

## sklearnìœ¼ë¡œ êµ¬í˜„í•˜ê³  ì„±ëŠ¥í‰ê°€ ì§„í–‰í•˜ê¸°
- BMI ì˜ˆì œ
- ì„±ëŠ¥í‰ê°€ì˜ metric - accuracy


```python
%reset

import numpy as np
import pandas as pd
from sklearn import linear_model
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from scipy import stats
import tensorflow as tf

# Raw Data Loading
df = pd.read_csv('./data/bmi.csv', skiprows=3)
display(df.head())
print(df.shape)
```

    Once deleted, variables cannot be recovered. Proceed (y/[n])? y
    


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>height</th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>188</td>
      <td>71</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>161</td>
      <td>68</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>178</td>
      <td>52</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>136</td>
      <td>63</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>145</td>
      <td>52</td>
    </tr>
  </tbody>
</table>
</div>


    (20000, 3)
    

### 1. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°


```python
# ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())  # ê²°ì¸¡ì¹˜ ì—†ìŒ
```

    label     0
    height    0
    weight    0
    dtype: int64
    


```python
# ì´ìƒì¹˜ í™•ì¸ : z-score ë°©ì‹
zscore_threshold = 2.0
print((np.abs(stats.zscore(df['height'])) > zscore_threshold).sum())  # => 0ì´ë©´ ì´ìƒì¹˜ê°€ ì—†ìŒ
print((np.abs(stats.zscore(df['weight'])) > zscore_threshold).sum())  # => 0ì´ë©´ ì´ìƒì¹˜ê°€ ì—†ìŒ
print(np.unique(df['label'], return_counts=True))  # ë°ì´í„° í¸í–¥ ì¡´ì¬ X
```

    0
    0
    (array([0, 1, 2], dtype=int64), array([6470, 5857, 7673], dtype=int64))
    

### 2. ì •ê·œí™”(Normalization) í•˜ê¸°


```python
# ë¨¼ì € train dataì™€ validation dataë¥¼ ë¶„ë¦¬í•œ í›„ ì •ê·œí™” ì§„í–‰
train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(df[['height', 'weight']],
                 df['label'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['label'])

scaler = MinMaxScaler()
scaler.fit(train_x_data)

norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)
```

### 3. Model ìƒì„± í›„ í•™ìŠµ ë° í‰ê°€


```python
model = linear_model.LogisticRegression(C=100000)
# ê·œì œ ì ìš©(L2 ê·œì œ)
# alphaê°’ ì •í•´ì•¼ í•¨ 
# ê·œì œ ê°•ë„ C = 1 / alpha

model.fit(norm_train_x_data, train_t_data)

# í‰ê°€ë¥¼ ìœ„í•œ ì˜ˆì¸¡ê²°ê³¼ ì–»ê¸°
predict_val = model.predict(norm_test_x_data)

# ë‚˜ì˜¨ ì˜ˆì¸¡ê²°ê³¼ì™€ test_t_data ë¹„êµí•˜ê¸°
acc = accuracy_score(predict_val, test_t_data)

print('sklearnìœ¼ë¡œ êµ¬í•œ Accuracy : {}'.format(acc))

# prediction
result = model.predict(scaler.transform(np.array([[187, 81]])))
print(result)  # í‘œì¤€
```

    sklearnìœ¼ë¡œ êµ¬í•œ Accuracy : 0.9845
    [1]
    

## Tensorflowë¡œ êµ¬í˜„í•˜ê³  ì„±ëŠ¥í‰ê°€ ì§„í–‰í•˜ê¸°
- BMI ì˜ˆì œ

### 1. batch ì²˜ë¦¬ X


```python
# multinomial ë¬¸ì œì´ë¯€ë¡œ label(train_t_data, test_t_data)ì„ one-hot encoding ì²˜ë¦¬
# tensorflowì˜ ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ ë³€ê²½ => tensorflow nodeë¡œ ìƒì„±

sess = tf.Session()

onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=3))  # depthëŠ” classì˜ ê°œìˆ˜
onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=3))    # depthëŠ” classì˜ ê°œìˆ˜

# tensorflow graph ê·¸ë¦¬ê¸°
X = tf.placeholder(shape=[None,2], dtype=tf.float32)
T = tf.placeholder(shape=[None,3], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([2,3]))
b = tf.Variable(tf.random.normal([3]))

# model, Hypothesis
logit = tf.matmul(X,W) + b
H = tf.nn.softmax(logit)

# loss function
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,
                                                                 labels=T))
# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)

# session, ì´ˆê¸°í™”
sess.run(tf.global_variables_initializer())

# ë°˜ë³µ í•™ìŠµ
for step in range(10000):
    _, loss_val = sess.run([train, loss], feed_dict={X:norm_train_x_data,
                                                     T:onehot_train_t_data})                           
    if step % 1000 == 0:
        print('loss value : {}'.format(loss_val))
```

    WARNING:tensorflow:From C:\Users\Public\Documents\ESTsoft\CreatorTemp\ipykernel_29596\2371570974.py:4: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
    
    WARNING:tensorflow:From C:\Users\Public\Documents\ESTsoft\CreatorTemp\ipykernel_29596\2371570974.py:10: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
    
    WARNING:tensorflow:From C:\Users\Public\Documents\ESTsoft\CreatorTemp\ipykernel_29596\2371570974.py:25: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.
    
    WARNING:tensorflow:From C:\Users\Public\Documents\ESTsoft\CreatorTemp\ipykernel_29596\2371570974.py:28: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.
    
    loss value : 1.1495620012283325
    loss value : 0.5054112076759338
    loss value : 0.40890854597091675
    loss value : 0.35897088050842285
    loss value : 0.32620033621788025
    loss value : 0.3023587465286255
    loss value : 0.28392332792282104
    loss value : 0.2690742611885071
    loss value : 0.2567571699619293
    loss value : 0.24631084501743317
    

### 2. batch ì²˜ë¦¬ O
- ë°˜ë³µ í•™ìŠµ ì‹œ ì£¼ì˜ì : í•™ìŠµë°ì´í„°ì˜ ì‚¬ì´ì¦ˆê°€ ë§¤ìš° í¬ë©´ ë©”ëª¨ë¦¬ì— ë°ì´í„°ë¥¼ í•œë²ˆì— ëª¨ë‘ loadingí•  ìˆ˜ ì—†ìŒ 
- -> memory faultë‚˜ë©´ì„œ ìˆ˜í–‰ ì¤‘ì§€ë¨
- => batchì²˜ë¦¬ë¡œ í•´ê²°


```python
# multinomial ë¬¸ì œì´ë¯€ë¡œ label(train_t_data, test_t_data)ì„ one-hot encoding ì²˜ë¦¬
# tensorflowì˜ ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ ë³€ê²½ => tensorflow nodeë¡œ ìƒì„±
sess = tf.Session()

onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=3))  # depthëŠ” classì˜ ê°œìˆ˜
onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=3))    # depthëŠ” classì˜ ê°œìˆ˜

# tensorflow graph ê·¸ë¦¬ê¸°
X = tf.placeholder(shape=[None,2], dtype=tf.float32)
T = tf.placeholder(shape=[None,3], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([2,3]))
b = tf.Variable(tf.random.normal([3]))

# model, Hypothesis
logit = tf.matmul(X,W) + b
H = tf.nn.softmax(logit)

# loss function
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,
                                                                 labels=T))
# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)

# session, ì´ˆê¸°í™”
sess.run(tf.global_variables_initializer())

# ë°˜ë³µ í•™ìŠµ
num_of_epoch = 1000  # í•™ìŠµì„ ìœ„í•œ ì „ì²´ epoch ìˆ˜
num_of_batch = 100   # í•œë²ˆì— í•™ìŠµí•  ë°ì´í„° ëŸ‰

for step in range(num_of_epoch):
    total_batch = int(norm_train_x_data.shape[0] / num_of_batch)
    
    for i in range(total_batch):
        batch_x = norm_train_x_data[i*num_of_batch:(i+1)*num_of_batch]
        batch_y = onehot_train_t_data[i*num_of_batch:(i+1)*num_of_batch]
        _, loss_val = sess.run([train, loss], 
                               feed_dict={X:batch_x,
                                          T:batch_y})                           
    if step % 100 == 0:
        print('loss value : {}'.format(loss_val))
```

    loss value : 0.7061227560043335
    loss value : 0.1640716791152954
    loss value : 0.12247326225042343
    loss value : 0.10330770164728165
    loss value : 0.09181509166955948
    loss value : 0.08399397879838943
    loss value : 0.0782523825764656
    loss value : 0.07381793856620789
    loss value : 0.07026541233062744
    loss value : 0.06734078377485275
    


```python
# í•™ìŠµ ì¢…ë£Œ í›„ ì„±ëŠ¥í‰ê°€(Accuracy)
result = sess.run(H, feed_dict={X:scaler.transform(np.array([[187,81]]))})
print(result)                     # ê°’ì´ ê°€ì¥ í° ê²ƒì´ ì´ ë°ì´í„°ì˜ class => 1
print(np.argmax(result, axis=1))  # ê°€ì¥ í° ê°’ì˜ index ì•Œë ¤ì¤Œ
```

    [[4.7410915e-05 9.0800029e-01 9.1952242e-02]]
    [1]
    


```python
predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(T,1))
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))

result = sess.run(accuracy, feed_dict={X:norm_test_x_data,
                                       T:onehot_test_t_data})
print(result)
```

    0.9855
    
