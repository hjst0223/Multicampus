# Logistic Regression êµ¬í˜„í•˜ê¸° ğŸ™„


```python
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn import linear_model  # sklearnìœ¼ë¡œ logistic êµ¬í˜„
from sklearn.preprocessing import MinMaxScaler  # ì •ê·œí™” ì§„í–‰
from scipy import stats  # ì´ìƒì¹˜ ì²˜ë¦¬
import matplotlib.pyplot as plt
import warnings

# ê²½ê³ ë©”ì‹œì§€ ì¶œë ¥ ì•ˆ ë˜ê²Œ ì„¤ì •
warnings.filterwarnings(action='ignore')

# Raw Data Loading
df = pd.read_csv('./data/admission.csv')
```

## Preprocessing

### 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬í•˜ê¸°


```python
# dfì˜ sum() : axis ìƒëµ ì‹œ (axis=1) columnë‹¹ í•© 
# numpyì˜ sum() : axis ìƒëµ ì‹œ ì „ì²´ í•©
print(df.isnull().sum())   # ê²°ì¹˜ê°’ ì—†ìŒ
df.info()
```

    admit    0
    gre      0
    gpa      0
    rank     0
    dtype: int64
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 400 entries, 0 to 399
    Data columns (total 4 columns):
     #   Column  Non-Null Count  Dtype  
    ---  ------  --------------  -----  
     0   admit   400 non-null    int64  
     1   gre     400 non-null    int64  
     2   gpa     400 non-null    float64
     3   rank    400 non-null    int64  
    dtypes: float64(1), int64(3)
    memory usage: 12.6 KB
    

### 2. ì´ìƒì¹˜ ì²˜ë¦¬í•˜ê¸°
- ì¢…ì†ë³€ìˆ˜ì˜ ì´ìƒì¹˜ : outlier
- ë…ë¦½ë³€ìˆ˜ì˜ ì´ìƒì¹˜ : ì§€ëŒ€ê°’
- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° outlierë¼ í•¨
- boxplot

#### boxplotìœ¼ë¡œ í™•ì¸í•˜ê¸°


```python
figure = plt.figure()
ax1 = figure.add_subplot(1,4,1)
ax2 = figure.add_subplot(1,4,2)
ax3 = figure.add_subplot(1,4,3)
ax4 = figure.add_subplot(1,4,4)
ax1.set_title('ADMIT')
ax2.set_title('GRE')
ax3.set_title('GPA')
ax4.set_title('RANK')

ax1.boxplot(df['admit'])
ax2.boxplot(df['gre'])
ax3.boxplot(df['gpa'])
ax4.boxplot(df['rank'])

# ë ˆì´ì•„ì›ƒ ì¡°ì ˆ
figure.tight_layout()

plt.show()  # ì´ìƒì¹˜ ì¡´ì¬
```


    
![png](/Machine-Learning/images/0404/output_7_0.png)
    


#### z-scoreë¡œ ì´ìƒì¹˜ ì œê±°í•˜ê¸°


```python
zscore_threshold = 2.0

for col in df.columns:
    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]
    df = df.loc[~df[col].isin(outlier)]
    
display(df)
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>admit</th>
      <th>gre</th>
      <th>gpa</th>
      <th>rank</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>380</td>
      <td>3.61</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>660</td>
      <td>3.67</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>800</td>
      <td>4.00</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>640</td>
      <td>3.19</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>520</td>
      <td>2.93</td>
      <td>4</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>394</th>
      <td>1</td>
      <td>460</td>
      <td>3.99</td>
      <td>3</td>
    </tr>
    <tr>
      <th>395</th>
      <td>0</td>
      <td>620</td>
      <td>4.00</td>
      <td>2</td>
    </tr>
    <tr>
      <th>396</th>
      <td>0</td>
      <td>560</td>
      <td>3.04</td>
      <td>3</td>
    </tr>
    <tr>
      <th>398</th>
      <td>0</td>
      <td>700</td>
      <td>3.65</td>
      <td>2</td>
    </tr>
    <tr>
      <th>399</th>
      <td>0</td>
      <td>600</td>
      <td>3.89</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>382 rows Ã— 4 columns</p>
</div>


### 3. ì •ê·œí™” ì‘ì—…í•˜ê¸°


```python
x_data = df.drop('admit', axis=1, inplace=False)
t_data = df['admit'].values.reshape(-1,1)  # t_dataëŠ” 0ê³¼ 1ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì •ê·œí™” í•„ìš” X

# ì •ê·œí™”ë¥¼ ìœ„í•œ scaler
scaler = MinMaxScaler()
scaler.fit(x_data)

norm_x_data = scaler.transform(x_data)
print(norm_x_data)
```

    [[0.04545455 0.71111111 0.66666667]
     [0.68181818 0.75555556 0.66666667]
     [1.         1.         0.        ]
     ...
     [0.45454545 0.28888889 0.66666667]
     [0.77272727 0.74074074 0.33333333]
     [0.54545455 0.91851852 0.66666667]]
    

#### training data set
- norm_x_data
- t_data

#### sklearn êµ¬í˜„


```python
model = linear_model.LogisticRegression()

model.fit(x_data, t_data)

my_score = np.array([[600, 3.8, 1]])
predict_val = model.predict(my_score)          # 0 or 1ë¡œ ê²°ê³¼ ë„ì¶œ
predict_proba = model.predict_proba(my_score)  # í™•ë¥ ê°’ìœ¼ë¡œ ê²°ê³¼ ë„ì¶œ

print('sklearnì˜ ê²°ê³¼ : í•©ê²©ì—¬ë¶€ : {}, í™•ë¥  : {}'.format(predict_val, predict_proba))
```

    sklearnì˜ ê²°ê³¼ : í•©ê²©ì—¬ë¶€ : [1], í™•ë¥  : [[0.43740782 0.56259218]]
    

#### tensorflow êµ¬í˜„


```python
# placeholder
X = tf.placeholder(shape=[None,3], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([3,1]))
b = tf.Variable(tf.random.normal([1]))

# Hypothesis, model, predict model, logistic regression model
logit = tf.matmul(X,W) + b  # linear regression
H = tf.sigmoid(logit)

# loss function, cross entropy, log loss
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# Session, ì´ˆê¸°í™”
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# ë°˜ë³µí•™ìŠµ
for step in range(300000):
    _, loss_val = sess.run([train, loss], 
                           feed_dict={X: norm_x_data, T: t_data})
    
    if step % 30000 == 0:
        print('lossì˜ ê°’ : {}'.format(loss_val))
```


    lossì˜ ê°’ : 1.1713826656341553
    lossì˜ ê°’ : 0.6711441874504089
    lossì˜ ê°’ : 0.632067084312439
    lossì˜ ê°’ : 0.6239297389984131
    lossì˜ ê°’ : 0.6186142563819885
    lossì˜ ê°’ : 0.6141023635864258
    lossì˜ ê°’ : 0.6101633310317993
    lossì˜ ê°’ : 0.606709361076355
    lossì˜ ê°’ : 0.6036775708198547
    lossì˜ ê°’ : 0.6010180711746216
    


```python
# predict
my_score = np.array([[600, 3.8, 1]])
norm_my_score = scaler.transform(my_score)

result = sess.run(H, feed_dict={X: norm_my_score})
print('tensorflowë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼ : {}'.format(result))
```

    tensorflowë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼ : [[0.4217303]]
    

# Regressionì˜ Metrics ğŸ˜‰


```python
import numpy as np
import pandas as pd
from sklearn import linear_model
from scipy import stats
from sklearn.model_selection import train_test_split


### Raw Data Loading
df = pd.read_csv('./data/ozone.csv')
# print(df.shape)  # (153, 6)

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
training_data = df.dropna(how='any', inplace=False)
# print(training_data.shape)  # (111, 6)

# ì´ìƒì¹˜ ì²˜ë¦¬
zscore_threshold = 2.0

for col in training_data.columns:
    outlier = training_data[col][np.abs(stats.zscore(training_data[col])) > zscore_threshold]
    training_data = training_data.loc[~training_data[col].isin(outlier)]
    
# sklearn => ì •ê·œí™” ì²˜ë¦¬í•  í•„ìš” ì—†ìŒ
# display(training_data.head())

# Data Set
x_data = training_data[['Solar.R', 'Wind', 'Temp']].values
t_data = training_data['Ozone'].values.reshape(-1,1)

# Train / Validation Data Set
train_x_data, valid_x_data, train_t_data, valid_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 random_state=2)  # randomì˜ seed ì—­í• 

# Model
model = linear_model.LinearRegression()

# Model í•™ìŠµ
model.fit(train_x_data, train_t_data)

# ì˜ˆì¸¡ê°’(predict_value)
# ì •ë‹µ(valid_t_data)
predict_value = model.predict(valid_x_data)

# ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µê°„ì˜ ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ
```

## 1. MAE(Mean Absolute Error) 
- ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ê°„ ì°¨ì˜ ì ˆëŒ€ê°’ì˜ í‰ê· 
- ì§ê´€ì , ë‹¨ìœ„ ê°™ìŒ
- scaleì— ë”°ë¼ ì˜ì¡´ì 


```python
from sklearn.metrics import mean_absolute_error

print(mean_absolute_error(valid_t_data, predict_value))
```

    13.924465776324642
    

## 2. MSE(Mean Squard Error)
- ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ê°„ ì°¨ì˜ ì œê³±ì˜ í‰ê· 
- MAEë³´ë‹¤ errorì— ê°€ì¤‘ì¹˜(ì œê³±)ì„ ì£¼ëŠ” Metrics
- MAEë³´ë‹¤ ì´ìƒì¹˜ì— ë” ë¯¼ê°í•¨


```python
from sklearn.metrics import mean_squared_error

print(mean_squared_error(valid_t_data, predict_value)) 
```

    271.56711923670605
    

## 3. R squared 
- ë¶„ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í‰ê°€ì§€í‘œ
- (ì˜ˆì¸¡ê°’ì˜ variance) / (ì •ë‹µì˜ variance)
- 1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ìˆìŒ


```python
from sklearn.metrics import r2_score

print(r2_score(valid_t_data, predict_value)) 
```

    0.3734728354920863
    
