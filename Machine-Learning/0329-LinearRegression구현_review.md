# Pythonìœ¼ë¡œ Simple Linear Regression êµ¬í˜„í•˜ê¸° ğŸ˜£


```python
import numpy as np

# Training Data Set ì¤€ë¹„
# x_data => ë…ë¦½ë³€ìˆ˜(ê³µë¶€ì‹œê°„)
x_data = np.array([1, 2, 3, 4, 5], dtype=np.float64).reshape(5,1)
# t_data => ì •ë‹µë°ì´í„°(ì‹œí—˜ì ìˆ˜)
t_data = np.array([3, 5, 7, 9, 11], dtype=np.float64).reshape(5,1)

# Weight & bias ì •ì˜
W = np.random.rand(1,1)  # 1í–‰ 1ì—´ì§œë¦¬(ê°’ì€ 1ê°œ) ndarrayë¥¼ ë§Œë“¤ê³ 
                         # 0ê³¼ 1ì‚¬ì´ì˜ ê· ë“±ë¶„í¬ì—ì„œ ì‹¤ìˆ˜ ë‚œìˆ˜ ì¶”ì¶œ
b = np.random.rand(1)     

# predict function (ì˜ˆì¸¡ í•¨ìˆ˜, ëª¨ë¸, hypothesis)
def predict(x):
    y = np.dot(x,W) + b
    
    return y

# loss function
def loss_func(input_data):  # loss í•¨ìˆ˜ëŠ” wì™€ bì˜ í•¨ìˆ˜
                            # input_data =>  [W b]
    
    input_W = input_data[0].reshape(1,1)
    input_b = input_data[1]
    
    # ì˜ˆì¸¡ê°’
    y = np.dot(x_data,input_W) + input_b
    
    # MSE(í‰ê· ì œê³±ì˜¤ì°¨)
    return np.mean(np.power(t_data-y,2))


# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def numerical_derivative(f,x):       # x : ndarray [1.0  2.0]
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)  # derivative_x : [0.0  0.0]
    
    # iteratorë¥¼ ì´ìš©í•´ì„œ ì…ë ¥ë³€ìˆ˜ xì— ëŒ€í•œ í¸ë¯¸ë¶„ ìˆ˜í–‰
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index 
        tmp = x[idx]            # tmp : 1.0
        
        x[idx] = tmp + delta_x  # x : ndarray [1.0001  2.0]
        fx_plus_delta = f(x)
        
        x[idx] = tmp - delta_x  # x : ndarray [0.9999  2.0]  
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
        
        x[idx] = tmp            #  x : ndarray [1.0  2.0]  
        it.iternext()
        
    return derivative_x

# learning rate ê°’ ì„¤ì • - í•™ìŠµì´ ì§„í–‰ë˜ëŠ” ê³¼ì •ì„ ë³´ë©´ì„œ ì ì ˆí•˜ê²Œ ìˆ˜ì •
# ì´ˆê¸°ì—ëŠ” 1e-4, 1e-3 ì •ë„ë¡œ ì„¤ì •í•´ì„œ ì‚¬ìš©
learning_rate = 1e-4

# í•™ìŠµê³¼ì • ì§„í–‰
for step in range(300000):
    
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)  # [W b]
    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)

    W = W - derivative_result[0].reshape(1,1)
    b = b - derivative_result[1]
    
    if step % 30000 == 0:
        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
        print('W : {}, b:{}, loss:{}'.format(W, b, loss_func(input_param)))
```

    W : [[0.48004985]], b:[0.29264637], loss:32.363935650389664
    W : [[2.02671433]], b:[0.90355278], loss:0.0016931387097876946
    W : [[2.00968806]], b:[0.96502302], loss:0.0002226782166688278
    W : [[2.00351342]], b:[0.98731545], loss:2.928619367812306e-05
    W : [[2.00127415]], b:[0.9953999], loss:3.851661617301118e-06
    W : [[2.00046208]], b:[0.99833175], loss:5.065628322144572e-07
    W : [[2.00016757]], b:[0.999395], loss:6.662213051946095e-08
    W : [[2.00006077]], b:[0.9997806], loss:8.762009355207204e-09
    W : [[2.00002204]], b:[0.99992043], loss:1.1523619453365784e-09
    W : [[2.00000799]], b:[0.99997114], loss:1.5155633825702596e-10
    

## í•™ìŠµ ì¢…ë£Œ í›„ ì˜ˆì¸¡í•˜ê¸°


```python
predict_date = predict(np.array([[6]]))
print('6ì‹œê°„ ê³µë¶€í–ˆì„ ë•Œ ì ìˆ˜ : ', predict_date)
```

    6ì‹œê°„ ê³µë¶€í–ˆì„ ë•Œ ì ìˆ˜ :  [[13.00000693]]
    

# sklearnìœ¼ë¡œ Simple Linear Regression êµ¬í˜„í•˜ê¸° ğŸ¤¨


```python
import numpy as np
from sklearn import linear_model  # sklearn ì„¤ì¹˜ 

# Training Data Set
x_data = np.array([1, 2, 3, 4, 5], dtype=np.float64).reshape(5,1)
t_data = np.array([3, 5, 7, 9, 11], dtype=np.float64).reshape(5,1)

# model ìƒì„±(Simple Linear Model)
model = linear_model.LinearRegression()

# model í•™ìŠµ
model.fit(x_data, t_data)

print('W: {}, b: {}'.format(model.coef_, model.intercept_))

# modelì„ ì´ìš©í•œ ì˜ˆì¸¡
print(model.predict(np.array([[6]])))
```

    W: [[2.]], b: [1.]
    [[13.]]
    

# OzoneëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ë§Œë“¤ê¸° ğŸ˜
- Ozone(ì˜¤ì¡´ëŸ‰) : ì¢…ì†ë³€ìˆ˜
- Solar.R(íƒœì–‘ê´‘ì„¸ê¸°), Wind(ë°”ëŒ), Temp(ì˜¨ë„) : ë…ë¦½ë³€ìˆ˜
- Simple Linear Regressionì´ë¯€ë¡œ ë…ë¦½ë³€ìˆ˜ 1ê°œë§Œ ì‚¬ìš© => 3ê°œì˜ ë…ë¦½ë³€ìˆ˜ ì¤‘ Tempë§Œ ì‚¬ìš©
- ì˜¨ë„ì— ë”°ë¥¸ ì˜¤ì¡´ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ë§Œë“¤ê¸°

## 1. Python êµ¬í˜„


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def numerical_derivative(f,x):       # x : ndarray [1.0  2.0]
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)  # derivative_x : [0.0  0.0]
    
    # iteratorë¥¼ ì´ìš©í•´ì„œ ì…ë ¥ë³€ìˆ˜ xì— ëŒ€í•œ í¸ë¯¸ë¶„ ìˆ˜í–‰
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index 
        tmp = x[idx]            # tmp : 1.0
        
        x[idx] = tmp + delta_x  # x : ndarray [1.0001  2.0]
        fx_plus_delta = f(x)
        
        x[idx] = tmp - delta_x  # x : ndarray [0.9999  2.0]  
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
        
        x[idx] = tmp            #  x : ndarray [1.0  2.0]  
        it.iternext()
        
    return derivative_x


# Raw Data Set Loading
df = pd.read_csv('./data/ozone.csv')
# display(df.head())

training_data = df[['Ozone', 'Temp']]
# display(training_data)
print(training_data.shape)  # (153, 2)

# ê²°ì¹˜ê°’ ì²˜ë¦¬
training_data.dropna(how='any', inplace=True)  # how='any' : ê²°ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” í–‰ ì‚­ì œ

# Training Data Set
x_data = training_data['Temp'].values.reshape(-1,1)
t_data = training_data['Ozone'].values.reshape(-1,1)

# Weight, bias
W = np.random.rand(1,1)
b = np.random.rand(1)

# loss function
def loss_func(input_data):
    W = input_data[0].reshape(1,1)
    b = input_data[1]
    
    y = np.dot(x_data,W) + b
    return np.mean(np.power(t_data-y,2))

# predict
def predict(x):
    y = np.dot(x,W) + b
    return y

# learning_rate
learning_rate = 1e-4

# í•™ìŠµê³¼ì • ì§„í–‰
for step in range(300000):
    
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)  # [W b]
    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)

    W = W - derivative_result[0].reshape(1,1)
    b = b - derivative_result[1]
    
    if step % 30000 == 0:
        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
        print('W : {}, b:{}, loss:{}'.format(W, b, loss_func(input_param)))
```

    (153, 2)
    W : [[0.58255823]], b:[0.89502333], loss:873.7397048431517
    W : [[0.71294764]], b:[-11.42338102], loss:819.1178219871285
    W : [[0.8558565]], b:[-22.71546398], loss:776.5805741102502
    W : [[0.98686218]], b:[-33.06700586], loss:740.834252736922
    W : [[1.10695612]], b:[-42.55634629], loss:710.7947096187999
    W : [[1.21704719]], b:[-51.25529983], loss:685.5508770767608
    W : [[1.31796855]], b:[-59.22969944], loss:664.3371362784469
    W : [[1.41048396]], b:[-66.53989472], loss:646.5100968259189
    W : [[1.49529357]], b:[-73.24120864], loss:631.5290834033376
    W : [[1.5730392]], b:[-79.38435618], loss:618.9397376645915
    

### í•™ìŠµ ì¢…ë£Œ í›„ ì˜ˆì¸¡í•˜ê¸°


```python
predict_data = predict(np.array([[62]]))
print('pythonìœ¼ë¡œ êµ¬í•œ ì˜¨ë„ê°€ 62ì¼ë•Œ ì˜¤ì¡´ëŸ‰ : {}'.format(predict_data))
```

    pythonìœ¼ë¡œ êµ¬í•œ ì˜¨ë„ê°€ 62ì¼ë•Œ ì˜¤ì¡´ëŸ‰ : [[16.93138375]]
    

### ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ê¸°


```python
plt.scatter(x_data.ravel(), t_data.ravel())
plt.plot(x_data.ravel(), x_data.ravel() * W.ravel() + b, color='r')
plt.show()
```


    
![png](/Machine-Learning/images/0329/output_13_0.png)
    


## 2. sklearn êµ¬í˜„


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model

df = pd.read_csv('./data/ozone.csv')
training_data = df[['Ozone', 'Temp']]
training_data.dropna(how='any', inplace=True)   # how='any' : ê²°ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” í–‰ ì‚­ì œ

# Training Data Set
x_data = training_data['Temp'].values.reshape(-1,1)
t_data = training_data['Ozone'].values.reshape(-1,1)

# model ìƒì„±
model = linear_model.LinearRegression()

# model í•™ìŠµ
model.fit(x_data, t_data)

# ì˜ˆì¸¡
result = model.predict(np.array([[62]]))
print('sklearnìœ¼ë¡œ êµ¬í•œ ì˜¨ë„ê°€ 62ë„ì¼ ë•Œì˜ ì˜¤ì¡´ëŸ‰ : {}'.format(result))  
```

    sklearnìœ¼ë¡œ êµ¬í•œ ì˜¨ë„ê°€ 62ë„ì¼ ë•Œì˜ ì˜¤ì¡´ëŸ‰ : [[3.58411393]]
    

### ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ê¸°


```python
plt.scatter(x_data.ravel(), t_data.ravel())
plt.plot(x_data.ravel(), 
         x_data.ravel()*model.coef_.ravel() + model.intercept_, color='g')
plt.show()
```


    
![png](/Machine-Learning/images/0329/output_17_0.png)
    

